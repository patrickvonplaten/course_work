\documentclass[xcolor=table,mathserif,9pt]{beamer}    % ,handout
% colortbl only defines \rowcolor for a single row. xcolor extends this to multiple rows.

\usetheme{Aachen}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
%\usepackage{tikz-dependency}
%\usepackage{chronology}
\usepackage{array, booktabs}
\newcommand{\foo}{\makebox[0pt]{\textbullet}\hskip+0.5pt\vrule width 5pt\hspace{\labelsep}}
\usepackage{multirow}
\usepackage{textcomp}
\usepackage{accents}
\newcommand{\ubar}[1]{\underaccent{\bar}{#1}}
\usepackage{media9}
\usepackage{csquotes}
\usepackage{amsmath}
\usepackage[]{algorithm2e}
\usepackage{lipsum}
\usepackage{multicol}
\usepackage{hyperref}
\setlength{\columnsep}{1cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% tables
\usepackage{multirow,array,tabularx,rotating}
\usepackage{booktabs}
\usepackage{tabularx}

% math
\usepackage{amsmath,amsthm, amssymb, latexsym, xspace}
%\usepackage{bbold}
\usefonttheme[onlymath]{serif}
\boldmath


% misc
\usepackage{subfig}
\usepackage{wasysym}
\usepackage{nameref}
\usepackage{xcolor}
\usepackage{romannum}
% declare the path(s) where your graphic files are
\graphicspath{{./nlu/}{./g2p/}{./smt/}}
% and their extensions so you won't have to specify these with
% every instance of \includegraphics
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}


%figures
\usepackage{tikz}
\usepackage{minibox}
% change the graphic extentions
%\usepackage{ifpdf}
%\ifpdf
%  \DeclareGraphicsExtensions{.pdf,.png,.jpg}
%\else
%  \DeclareGraphicsExtensions{.eps}
%\fi

\usepackage[np,autolanguage]{numprint}
\nprounddigits{1}

% helpers
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\sign}{\operatornamewithlimits{sign}}
\newcommand{\Eqn}{Equation}
\newcommand{\Eqns}{Equations}
\newcommand{\Fig}{Figure}
\newcommand{\Figs}{Figures}
\newcommand{\Tab}{Table}
\newcommand{\Sec}{Section}
\def\example{{\textit{}{e.g.}}\xspace}
\def\cad{{\textit{}{i.e.}}\xspace}
\def\etc{{\textit{etc.}}\xspace}
\def\apriori{{\textit{a priori}}\xspace}

\newcommand{\NetTalk}{NETtalk\xspace}
\newcommand{\Celex}{Celex\xspace}
\newcommand{\Pronlex}{Pronlex\xspace}
\newcommand{\gtp}{G2P}
\newcommand{\Seg}{\mathbb{S}}
\newcommand\BLEU{\textsc{Bleu}\xspace}
\newcommand\TER{\textsc{Ter}\xspace}
\newcommand\CTER{\textsc{CTer}\xspace}

\newcommand\AER{\textsc{Aer}\xspace}
\newcommand\SAER{\textsc{Saer}\xspace}
\newcommand{\GIZA}{{GIZA\nolinebreak[4]\hspace{-.025em}\raisebox{.2ex}{\small\bf++}}\xspace}

\newcommand{\todo}[1]{\colorbox{yellow}{#1}\xspace}
\newcommand{\CITE}{\colorbox{yellow}{CITE}\xspace}
\newcommand{\REF}{\colorbox{yellow}{REF}\xspace}
\newcommand{\EQ}{\colorbox{yellow}{REF}\xspace}
\newcommand{\NUMBER}{\colorbox{yellow}{NUMBER}\xspace}
\newcommand{\Align}{\mathbb{A}}
%\renewcommand{\emph}[1]{\textcolor{i6blue}{#1}}


\newcommand{\aind}[1]{\hspace*{#1ex}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}

\newcommand*{\sumw}{\ensuremath{\operatornamewithlimits{sum}}\xspace}
\newcommand*{\nbest}{\ensuremath{\operatornamewithlimits{nbest}}\xspace}

% Define box and box title style
\usepackage{tikz}
\usetikzlibrary{shapes,positioning,fit}
\tikzstyle{bubble} = [ draw=blue, rectangle, rounded corners, inner sep=3pt, inner ysep=
10 pt]
\tikzstyle{fancytitle} =[fill=white, text=black]
\newenvironment{bubble}[3]{%
  \begin{tikzpicture}[transform shape, baseline=-0.5 cm]
  \def\bubbletitle{#1}%
  \node [bubble] (box)\bgroup
  \begin{minipage}[t][#3]{#2}%
}{%
  \end{minipage}%
  \egroup;
  \node[fancytitle] at (box.north) {\bubbletitle};
  \end{tikzpicture}%
}

\newenvironment{bubble*}[2]{%
  \begin{tikzpicture}[transform shape]
  \node [bubble] (box)\bgroup
  \begin{minipage}[t][#2]{#1}%
}{%
  \end{minipage}%
  \egroup;
  \end{tikzpicture}%
}

% CUSTOM STUFF %%%%%%%%%%%%%%%%%
%\usepackage{neuralnetworks}


% Rounding commands
\def\roundpositionii{1}
\newcommand{\rdmii}[1]{\edef\rounded{0}\FPeval\rounded{round(#1,\roundpositionii)}\rounded}
\def\roundpositioni{1}
\newcommand{\rdmi}[1]{\edef\rounded{0}\FPeval\rounded{round(#1,\roundpositioni)}\rounded}
\def\roundposition{0}
\newcommand{\rdm}[1]{\edef\rounded{0}\FPeval\rounded{round(#1,\roundposition)}\rounded}

\npstyleenglish

\def\roundposition{1}
\def\roundpositiont{3}
\edef\rounded{0}
\newcommand{\rd}[1]{\ifthenelse{\equal{#1}{--}}{--}{\edef\rounded{0}\FPeval\rounded{round(#1,\roundposition)}\rounded}}
\newcommand{\rdmt}[1]{\edef\rounded{0}\FPeval\rounded{round(#1,\roundpositiont)}\rounded}
\newcommand{\rp}[1]{%
  \FPset{\per}{#1}%
  %\FPmul{\per}{\per}{100}%
  \FPround{\per}{\per}{1}%
  \numprint{\per}%
}%


\setbeamercovered{transparent}

\DeclareMathOperator*{\sigmoid}{sigmoid}

\newcommand\T{\rule{0pt}{2.2ex}}       % Top strut
\usepackage{etoolbox}
\pretocmd{\section}{\addtocontents{toc}{\vspace{-20pt}}}{}{}

%Table stuff
\newcommand{\tablesize}{}
\newcommand{\abovetable}{\vspace{0.6cm}}
\newcommand{\belowtable}{\vspace{0.13cm}}


% Get fond size for system combination picture right
\usepackage{fix-cm}    
\makeatletter
\newcommand\SyscomFontSize{\@setfontsize\small{6}{60}}
\makeatother   

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\backupbegin}{
   \newcounter{finalframe}
   \setcounter{finalframe}{\value{framenumber}}
}
\newcommand{\backupend}{
   \setcounter{framenumber}{\value{finalframe}}
}

\newcommand{\stoptocwriting}{%
  \addtocontents{toc}{\protect\setcounter{tocdepth}{-5}}}
\newcommand{\resumetocwriting}{%
  \addtocontents{toc}{\protect\setcounter{tocdepth}{2}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand*{\email}{\url{patrick.platen@rwth-aachen.de}}  
% all email address(es) of the authors (used for \TitlePage)

\title[Seminar]{Deep Learning for the Physical Layer}
%\subtitle{Presentation Subtitle} % (optional)
%\setbeamertemplate{section in toc}[sections numbered] 
%\setbeamertemplate{subsection in toc}[subsections numbered] 
\setbeamertemplate{navigation symbols}{} %disable {, dotted}navigation bar

%% author and in []: shortauthorChange the scale of beamer from 3.5 to 1
\author[Patrick von Platen]{Patrick von Platen}
% - Use the \inst{?} command only if the author7s have different
%   affiliation.
\institute[RWTH Aachen University] % (optional, but mostly needed)
{
%  \inst{1}%
  \strut Institute for Theoretical Information Technology\\
  \strut Univ.-Prof. Dr. rer. nat. Rudolf Mathar%\\
  %\strut {\tt lehnen@cs.rwth-aachen.de}
}
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.

\date[19/7/2018]{July 19th, 2018}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%shit
% will be set into the PDF document summary
\hypersetup{
  pdftitle={\inserttitle}, 
  pdfauthor={\insertauthor}, 
  bookmarksdepth=subsubsection,  
  % enable automatic page transitions: for endless loop edit in
  % acrobat reader -> preferences -> full screen -> after every X
  % seconds and after last page
  %pdfpageduration = 2, 
  % pdfpagetransition = {Glitter /Di 315 /D 5}  
  % pdfpagetransition = {Box /M /O /D 1},
} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[label=titlepage]
  \titlepage
\end{frame}

\begin{frame}
	\frametitle{Outline}
	\tableofcontents
        %\tableofcontents[currentsection, subsectionstyle=show/show/hide]
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Literature}%
\label{sec:literature}


\begin{frame}{Literature}
\begin{description}
\item [Timothy J. O'Shea and Jakob Hoydis:] An introduction to machine learning commu- nications systems. {\em Intro to DL for CommSystems 2017}.
  \begin{itemize}
  \item Autoencoder as Communication System 
  \end{itemize}
\end{description}
\begin{description}
\item [T. J. O'Shea, T. Roy, N. West, and B. C. Hilburn:] Physical Layer Communications System Design Over-the-Air Using Adversarial Networks. {\em Adversarial Networks for CommSystems 2018}.
  \begin{itemize}
  \item Generative Adversial Networks for Communication Systems 
  \end{itemize}
\end{description}
\begin{description}
\item [Alexander Felix, Sebastian Cammerer, Sebastian Doerner] Ofdm-autoencoder for end-to-end learning of communications systems. {\em OFDM Autoencoder 2018}.
  \begin{itemize}
  \item Synchronization in Autoencoders 
  \end{itemize}
\end{description}
\begin{description}
\item [Ian Goodfellow, Yoshua Bengio, and Aaron Courville:] Deep Learning. {\em Basics for Deep Learning 2016}.
  \begin{itemize}
  \item Basics in Machine Learning 
  \end{itemize}
\end{description}
\end{frame}

\section{Introduction}%
\label{sec:introduction}
\begin{frame}{Introduction - Deep Learning for the physical layer}

\begin{figure}[htpb]
	\centering
	\includegraphics[width=.6\textwidth]{images/CommunicationSystem.png}
	\caption{Block diagram for conventional physical layer \cite{CommTech}}
\end{figure}

\centering{\underline{Motivation for applying deep learning to the physical layer}}
\vspace{1em}
\begin{itemize}
	\item Rise of deep learning in multiple different fields of research
	\item Data needed for training can be generated since input data equals output data
	\item Conventional systems are optimized block by block and not as one
\end{itemize}


\end{frame}

\section{Source and Channel Encoding}%
\begin{frame}{Source and Channel Encoding}

\centering{\underline{Source Encoding}}
\vspace{1em}
\begin{itemize}
	\item Symbols to be encoded to efficient code: $x(k) \to \ubar{X}$
	\item Remove redundancy in code (Shannon code theory)
	\item \emph{Huffman-Coding, Run-Length-Coding, Arithmetic-Coding,...}
\end{itemize}

\vspace{2em}

\centering{\underline{Channel Encoding}}
\vspace{1em}
\begin{itemize}
	\item Source encoded bits to channel encoded bits: $\ubar{X} \to \ubar{Y}$
	\item Redundancy is added to bits to make them more error resistant
	\item \emph{Hamming code, Convolutional encoding, Turbo-Code,...}
\end{itemize}

\end{frame}

\section{Modulation}%
\begin{frame}{IQ - Modulation}

\begin{itemize}
	\item Channel encoded bits modulated into carrier wave $\ubar{Y} \to u(t)$
	\item Two values $\ubar{Y}_1$ and $\ubar{Y}_2$ are modulated as \emph{I/Q values} into the amplitude of two carrier 
	      waves $s_c$ of same frequency $f_o$ with a phase-shift $\frac{\pi}{2}$
	\begin{itemize}
		\item $s_c(t) = \Re{(I(t) + iQ(t))e^{-2\pi f_0 t}}$
		\item ... $ = \Re{(A(t)e^{-2\pi i f_o t + \varphi(t) i})}$ with $I(t) = A(t)\cos(\varphi(t))$ and $Q(t) = A(t)i\sin(\varphi(t))$
	\end{itemize}
	\item IQ - Modulation is essentially the same as combined modulation of phase and amplitude
	\item Modulation scheme can be viewed in complex plane
\end{itemize}

\begin{figure}[htpb]
	\centering
	\includegraphics[width=0.35\linewidth]{images/16QAM.png}
	\caption{16 Quadrature Amplitude Modulation \cite{CommTech}}
	\label{fig:16QAM}
\end{figure}
\end{frame}

\section{Physical Channel and Receiver}
\begin{frame}{Source and Channel Encoding}

\centering{\underline{Physical Channel}}
\vspace{1em}
\begin{itemize}
	\item Carrier wave is transmitted via medium $u(t) \to v(t)$
	\item $v(t) = u(t) + n(t)$ with $n(t)$ being some noise caused by interference. 
	\item $n(t)$ is mostly modelled by gaussian noise.
\end{itemize}

\vspace{2em}

\centering{\underline{Receiver}}
\vspace{1em}
\begin{itemize}
	\item Received carrier wave to decoded source symbols $v(t) \to \hat{x(t)}$
	\item Demodulated code $\ubar{Z}$ is encoded using methods such as \emph{minimum distance decoding, syndrome decoding, maximum
		likelihood Decoding}$\to \ubar{\hat{X}}$
	\item $\ubar{\hat{X}}$ is decoded back to $\hat{x}(t)$ 
\end{itemize}

\end{frame}

\section{Autoencoder}
\begin{frame}{Autoencoder}

	\begin{figure}[htpb]
		\centering
		\includegraphics[width=0.6\linewidth]{images/autencod.png}
		\caption{Autoencoder}
	\end{figure}

\begin{itemize}
	\item Encoder: $f_e: x \to y, |y| < |x|$ maps input to a \emph{code} that 
	      has a lower dimension than the input.
	\item Decoder: $f_d: y \to \hat{x}$ maps code to the output.
	\item Network tries to learn compact representation of input (code) and tries 
	      to reconstruct input from the code.
	\item Network is trained on input data. Loss function $L_a = L(f_d(f_e(x)),x)$              computes difference between input and reconstructed input.
\end{itemize}
\end{frame}

\section{Autoencoder as Communication System}
\begin{frame}{Autoencoder as Communication System (1)}

\centering{\underline{Applying an autoencoder to a communication system}}
\vspace{1em}
\begin{figure}[htpb]
	\centering
	\includegraphics[width=0.8\linewidth]{images/IntroCommunicationSystemAsNeuralNetwork.png}
\end{figure}

\end{frame}

\begin{frame}{Autoencoder as Communication System (2)}

\begin{figure}[htpb]
	\centering
	\includegraphics[width=0.8\linewidth]{images/communicationSystemAsAutoencoder.png}
	\caption{Autoencoder as Communication System \cite{synch1}}
	\label{fig:autoencoder}
\end{figure}

\begin{itemize}
	\item Transmitter Neural Network (NN): \emph{$f_{\theta_t}: s \to x$} with $s \in \mathbb{R}^m, x \in \mathbb{R}^n$
	\begin{itemize}
		\item $s = \left[s_1,...,s_n\right]$: one-hot encoded vector with $s_i$ being the i-th symbol 
	\end{itemize}
	\item Channel NN: \emph{$f_{c, \sigma^2}: x \to y$} with $y \in \mathbb{R}^n$
	\begin{itemize}
		\item $x \in \mathbb{R}^n$: $n$ values to be modulated into carrier wave, $y \in \mathbb{R}^n$ received demodulated values
	\end{itemize}
	\item Receiver NN: \emph{$f_{\theta_r}: y \to \hat{s}$} with $\hat{s} \in \mathbb{R}^m$
	\begin{itemize}
		\item $\text{argmax}_i\hat{s}$ gives decoded symbol
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Autoencoder as Communication System (3)}

	\centering{\underline{Architecture}}
	\vspace{1em}
	\begin{itemize}
		\item $f_{\theta_t}$: contains multiple hidden layers, an output layer $ \in \mathbb{R}^n$ and a normalization layer, having one or more contraints:
		\begin{itemize}
			\item amplitude constraint $|x_i| \le a, \forall i$, average power constraint $E(|x_i|^2) \le p, \forall i$ 
		\end{itemize}
		\item $f_{c, \sigma^2}: x \to x + v = y$ with $v \sim \mathcal{N}(0,\,\sigma^{2})$
		\item $f_{\theta_r}$: contains multiple hidden layers, output layer with \emph{softmax function $\hat{s}_i = \frac{e^{s_i^{'}}}{\sum_{j=1}^m e^{s_j^{'}}}$ 
			with $s^{'}$ output of the output layer before applying softmax}
	\end{itemize}

	\vspace{1em}
	\centering{\underline{Training}}
	\vspace{1em}
	\begin{itemize}
		\item $f_{c, \sigma^2}$ is needed for training so that transmitter NN and receiver NN can be trained together
		\item Training is done using gradient descent:
			\begin{itemize}
				\item $\theta_t \leftarrow \theta_t - \alpha \frac{\partial L(f_{\theta_r}(f_{c, \sigma^2}(f_{\theta_t}(s))),s)}{\partial \theta_t}$ 
				\item $\theta_r \leftarrow \theta_r - \alpha \frac{\partial L(f_{\theta_r}(y),s)}{\partial \theta_r}$ 
				\item \emph{learning rate} $\alpha$ and \emph{loss function} $L$
			\end{itemize}
		
	\end{itemize}

\end{frame}

\begin{frame}{Autoencoder as Communication System (4)}

	\begin{itemize}
		\item Autoencoder is able to learn \emph{efficient modulation schemes}
	\end{itemize}
	\vspace{1em}

\begin{figure}
  \centering
	\subfloat[Learned Modulation Scheme for n=2, m=16 with Energy constraint \label{fig:a}]{\includegraphics[height=7cm,width=9cm]{images/learnedModulation.png}}\qquad
  \subfloat[Learned Modulation Scheme for n=2, m=16 with Amplitude constraint\label{fig:b}]{\includegraphics[height=7cm,width=9cm]{images/learnedModulation2.png}}
\end{figure}

\begin{itemize}
	\item Transmitter NN parameters $\theta_t$ is used at transmitter for encoding
	\item Receiver NN parameters $\theta_r$ is used at receiver for decoding
	\item Modulation of $x = f_{\theta_t}$ into carrier wave and demodulation coupled with synchronization is not taken care of 
	      using basic approach.	
	\item Channel layer approximation $f_{c, \sigma^2}$ is sometimes modelling real world channel behaviour badly.
\end{itemize}


\end{frame}

\section{Generative Adversial Networks for Communication Systems}
\begin{frame}{Generative Adversial Networks for Comm. Systems (1)}

\begin{figure}[htpb]
	\centering
	\includegraphics[width=0.85\linewidth]{images/generativeAdvAutoencoder.png}
	\caption{Generative Adversial Networks for Comm. Systems \cite{2018arXiv180303145O}}
\end{figure}

\begin{itemize}
	\item Channel Approximation $h_{I, \theta_h}: x \to y$ 
	\item Real world channel $h_0: x \to y$
	\item Transmitter $f_{\theta_f}: s \to x $
	\item Receiver $g_{\theta_g}: y \to \hat{s}$
\end{itemize}

\end{frame}

\begin{frame}{Generative Adversial Networks for Comm. Systems (2)}

\begin{figure}[htpb]
	\centering
	\includegraphics[width=0.7\linewidth]{images/trainingAlgo.png}
	\caption{Training Procedure Generative Adversial Networks for Communication System}
\end{figure}

\begin{itemize}
	\item $\alpha_0$ is learning rate
	\item $L_0$ is loss function comparing 
		channel approximation output $y_k$ and real world output $\hat{y_k}$ using \emph{squared mean loss}
	\item $L_1$ is loss function comparing input $s_k$ and output $\hat{s_k}$ using \emph{cross entropy loss}
\end{itemize}

\end{frame}

\section{Synchronization in Autoencoders}
\begin{frame}{Synchronization in Autoencoders (1)}

\begin{figure}[htpb]
	\centering
	\includegraphics[width=0.5\linewidth]{images/synchBlock.png}
	\caption{Autoencoder for Communication Systems with Synchronization \cite{synch1}} 
\end{figure}

\begin{itemize}
	\item Autoencoder system is extended by four new blocks to a \emph{OFDM} like system:
		\begin{itemize}
			\item ``IFFT'' block performing an inverse fourier transform
			\item ``CP Add'' block adding a cyclic prefix as pilot symbol
			\item ``CP Synch'' block synchronizing the received carrier wave by detecting the cyclic prefix
			\item ``FFT'' block performing a fourier transform
		\end{itemize}
	\item ``CP Synch'' block is made up of NN having trainable parameters and that is \emph{trained jointly} with ``Transmitter NN'' and ``Receiver NN''
	\item ``IFFT'', ``CP Add'' and ``FFT'' have no trainable parameters 
\end{itemize}

\end{frame}

\begin{frame}{Synchronization in Autoencoders (2)}

\begin{figure}[htpb]
	\centering
	\includegraphics[width=0.5\linewidth]{images/ifft_synch.png}
	\caption{FFT for Synchronization in autoencoders \cite{synch1}}
\end{figure}

\begin{itemize}
	\item A single message $s$ is represented by $\frac{n}{2}$ complex-valued I/Q symbols.
	\item $w_{\text{FFT}}$ independent autoencoder symbols $x$ form a single \emph{OFDM symbol} $X_{\text{OFDM}} \in \mathbb{C}^{\frac{n}{2} \times w_{\text{FFT}}}$
	\item After every $n$ complex-valued I/Q symbols (2 complete messages) a cyclic prefix is added so $X_{2 \times\text{OFDM,CP}} \in \mathbb{C}^{(n + 1) \times w_{\text{FFT}}}$
	\item Training should be done in batches over $\ge 2w_{\text{FFT}}$ complete messages including a cyclic prefix
\end{itemize}



\end{frame}

\begin{frame}{Results}

\begin{figure}[htpb]
	\centering
	\includegraphics[width=0.4\linewidth]{images/BLERComp.png}
	\caption{Autoencoder and 3 Baseline Conventional Communication Schemes Comparison using BLER \cite{DBLP:journals/corr/OSheaH17}}
\end{figure}

\begin{itemize}
	\item energy per bit to noise power: $\frac{E_b}{N_0}$
	\item block error rate (BLER): $\frac{\text{Erroneous Blocks}}{\text{Total Blocks}}$
	\item Autoencoder $(7,4)$ has $n=7$ channels and $m=2^4=16$ possible input nodes
	\item Comparison with Hamming $(7,4)$ codes (hard decision coding and maximum likelihood decoding) and 
	      binary phase-shift keying modulation (BPSK)
\end{itemize}
\end{frame}


\section{Conclusion}
\begin{frame}{Conclusion}

\begin{itemize}
	\item It has been shown that communication systems using autoencoder architectures perform as good or better than conventional systems
	\item Autoencoders always must be trained and tuned before there can be applied 
	\item Still very young field of research with many new architectures and methods invented in the last two years
\end{itemize}

\end{frame}

\begin{frame}[label=finalSlide]
  \label{LastPage}%
  \begin{center}
    \vfill
    {\Large
    \textcolor{i6bluedark}{Thank you for your attention}
    }
%     \vfill
%     \inserttitle
    \vfill
    {\insertauthor}

    \vspace{10mm}
    \url{patrick.platen@rwth-aachen.de}
  \end{center}
\end{frame}

\nocite{*}

\stoptocwriting
\section{Appendix}
\resumetocwriting

%\backupbegin


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Appendix}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[allowframebreaks]
  \centerline{Reference}
 %\bibliographystyle{ieeetr}
 \bibliographystyle{i6bibstyle}
 \bibliography{references}
\end{frame}

%\backupend

\end{document}

