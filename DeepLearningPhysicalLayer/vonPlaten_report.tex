\documentclass[twoside,11pt,a4paper,english]{article}

% packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx,curves,float,rotating}

\usepackage{amsmath, amssymb, latexsym}  % math stuff
\usepackage{amsopn}                             % um mathe operatoren zu deklarieren
\usepackage[english]{babel}                     % otherwise use british or american
\usepackage{theorem}                            % instead of \usepackage{amsthm}
\usepackage{dcolumn}
\usepackage{hyperref}
\usepackage[]{algorithm2e}
\usepackage{accents}
\newcommand{\ubar}[1]{\underaccent{\bar}{#1}}




% @ environment %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{xspace}                             % context sensitive space after macros
\makeatletter 
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{{e.g}\onedot} \def\Eg{{E.g}\onedot}
\def\ie{{i.e}\onedot} \def\Ie{{I.e}\onedot}
\def\cf{{c.f}\onedot} \def\Cf{{C.f}\onedot}
\def\etc{{etc}\onedot} \def\vs{{vs}\onedot} 
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{{et al}\onedot}
\def\zB{z.B\onedot} \def\ZB{Z.B\onedot}
\def\dh{d.h\onedot} \def\Dh{D.h\onedot}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%	Macros fuer neue Umgebungen
  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand*{\Frac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}
\newlength{\textwd}
\newlength{\oddsidemargintmp}
\newlength{\evensidemargintmp}
\newcommand*{\hspaceof}[2]{\settowidth{\textwd}{#1}\mbox{\hspace{#2\textwd}}}
\newlength{\textht}
\newcommand*{\vspaceof}[3]{\settoheight{\textht}{#1}\mbox{\raisebox{#2\textht}{#3}}}
\newcommand*{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}

\newenvironment{deflist}[1][\quad]%
{  \begin{list}{}{%
      \renewcommand{\makelabel}[1]{\textbf{##1}\hfil}%
      \settowidth{\labelwidth}{\textbf{#1}}%
      \setlength{\leftmargin}{\labelwidth}
      \addtolength{\leftmargin}{\labelsep}}}
{  \end{list}}


\newenvironment{Quote}% Definition of Quote
{  \begin{list}{}{%
      \setlength{\rightmargin}{0pt}}
      \item[]\ignorespaces}
{\unskip\end{list}}


\theoremstyle{break}
\theorembodyfont{\itshape}	
\theoremheaderfont{\scshape}

\newtheorem{Cor}{Corollary}
\newtheorem{Def}{Definition}
%\newtheorem{Def}[Cor]{Definition}



\newcolumntype{.}{D{.}{.}{-1}}


\pagestyle{headings}
\textwidth 15cm
\textheight 23cm
\oddsidemargin 1cm
\evensidemargin 0cm
%\parindent 0mm



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%       Jetzt geht's los
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%               Title
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{empty}

\begin{center}

    Rheinisch-Westf\"alische Technische Hochschule Aachen \\
    Institute for Theoretical Information Technology \\
    Univ.-Prof. Dr. rer. nat. Rudolf Mathar \\[6ex]
    Seminar on Deep Learning - Methodologies and Applications - SS2018\\[12ex]                          % auch Seminar Titel und Datum ändern!!!
   
    \LARGE
    \textbf{Deep Learning for the Physical Layer} \\[6ex]
    \textit{Patrick von Platen} \\[6ex]
    \Large
    Matrikelnummer 331 430 \\[6ex]
    Datum des Vortrages

    \vfill
    \Large Betreuer: Johannes Schmitz 
	    
\end{center}

\newpage
\ 
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%               Inhaltsverzeichnis / Tabellenverzeichnis / Abbildungsverz.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{headings}
\tableofcontents
\listoftables
\listoffigures
\newpage
\pagestyle{empty}
\ 
\newpage
\pagestyle{headings}

\section{Motivation}%
\label{sec:motivation}

The physical layer in communication systems defines the means of reliably transmitting 
data over a physical link (called \emph{channel}) connecting network nodes.
Since communications is a complex and mature engineering field, large performance 
improvements, especially in the physical layer, have become rare \cite{DBLP:journals/corr/OSheaH17}.

One of the guiding principles of communication system design is to divide the signal processing into a chain
of multiple independent blocks as can be seen in \ref{fig:BlockCommunicationSystem}. 
Each block (e.g., source/channel coding, modulation, equalization) executes a well defined and isolated function. 
Although this approach has led to the efficient, versatile, and controllable systems we have today, it is not clear that individually optimized processing
blocks achieve the best possible end-to-end performance \cite{DBLP:journals/corr/OSheaH17}.
Today, it is known that the separation of communicaton system design into multiple 
individual blocks is \emph{sub-optimal} for many practical 
channels \cite{141453,504941}.

Continuous rise of deep learning in other fields recently and possible performance 
improvments motivated researchers, such as \emph{T. J. O'Shea}, to apply artificial neural networks to model end-to-end communication systems using autoencoders \ref{sub:autoencoder}. 

In this report, we want to study how conventional systems of the physical layer can 
be modelled by autoencoders. 
In section \ref{sec:communication_systems}, we will introduce the conventional
communication systems. Then we will give an overview of the most important deep learning 
methods \ref{sec:deep_learning_basics}, that are used to model end-to-end systems 
using autoencoders. 
Two of these systems, one having a closed analytical form by modelling the channel as gaussian 
noise, the other one having a open analytical form using generative adversarial networks \ref{sub:generative_adversarial_networks} to model an unknown channel response will be studied in detail in section 
\ref{sec:applying_deep_learning_to_communication_systems}. 
Finally a conclusion will be drawn \ref{sec:conclusion}.

\section{Conventional Communication Systems} 
\label{sec:communication_systems}

This section aims at explaining what the so-called physical layer is actually made of 
by describes it function and structure in detail. Later in this section, an example system will be explained and used for performance comparasion with communcation systems applying deep learning methods \ref{sec:applying_deep_learning_to_communication_systems}.

In this report, we use the word \emph{communication system} as the \emph{Physical Layer} of the well-know \emph{OSI Model} as a \emph{communication system}.
As it is defined in \cite{osimodel}, ``the physical layer manages the reception and transmission of the unstructured raw bit stream over a physical medium''. 
The physical layer therefore comprises transforming the raw bit stream to electromagnetic waves that will be transmitted by an antenna at the transmitter, receiving the 
electromagnetic waves via an antenna at the receiver and using them to recover the raw bit stream. 
In the following the connection between the antenna of the transmitter and the antenna of
the receiver will be defined as the \emph{channel}.

\subsection{Structure of the Communication System}%
\label{sub:structure_of_the_communication_system}



The role of the communication system is too two-fold:

\begin{itemize}
	\item Efficient Transmitting: The raw bit stream should be converted in such a 
		way that it can be transmitted at a high bit rate in terms of the \emph{Shannon information theory} \cite{Shannon:2001:MTC:584091.584093}
	\item Reliable Delivery: In case of errors to due noise in the channel, the 
		communication system must be able to do error detection and error correction. 
\end{itemize}

This can be achieved by applying sophisticated encoding and decoding schemes also just called ``codes'' to the raw bit stream. Such a ``code'' would for example be the so-called
``Turbo-Codes'' \cite{Berrou93nearshannon} that use a special encoding and decoding schemeto reach ``near Shannon limit error-correcting coding and decoding'' meaning that the 
maximum rate at which information can be transmitted over a channel of a specified 
bandwidth in the presence of noise, being the \emph{Shannon rate}, can nearly be archieved while the methods allows for error correction.

\begin{figure}[htpb]
	\centering
	\includegraphics[width=0.8\linewidth]{images/CommunicationSystem.png}
	\caption{Block Diagram Communication System}
	\label{fig:BlockCommunicationSystem}
\end{figure}

The figure above \ref{fig:BlockCommunicationSystem} describes the 
traditional structure of communication systems. 
We divide the communication system into three parts being the \emph{transmitter,
the channel and the receiver}.
We define all blocks leading to the physical channel to be part of the transmitter, 
including the \emph{source encoder, the channel encoder and the modulator}. The channel, 
called the ``physical channel'' in \ref{fig:BlockCommunicationSystem}, is considered as a separate part, and lastly the following parts \emph{equilizer/demodulator, channel decoder and the source decoder} are part of the receiver. 
In the following, in every part it is shortly explained which transformations are 
applied to the raw bit stream in conventional systems to better understand how \emph{deep
learning methods} can be used to replace them \ref{sec:applying_deep_learning_to_communication_systems}. A deeper explanation is out of scope for this report. 

\subsubsection{Transmitter}

First, in the block \emph{source encoding}, the raw bit stream $x(k)$ is encoded to 
reduce redundancy. 
For this, a source coding scheme is used that defines both how to encode and decode 
a set of symbols $S$ with $x(k) \in S$.

Assuming for example that every symbol in $x(k)$ has the same bit 
length, source coding makes sure that symbols that appear with a higher probability 
are encoded to a more efficent presentation containing much less bits whereas 
symbols appearing with a low probability are encoded to symbols of a higher bit rate.
This way, the overall average \emph{bit rate} can be optimized. There are multiple 
procedures, like \emph{Run-length encoding, Huffman encoding, arithmetic coding,...} from which we can choose from. 
Depending on the coding scheme, both the transmitter and the receiver must know which coding is used to work together.

Second. in the block \emph{channel encoding}, the efficient source code presentation $\ubar{X}$ is now transformed to a more error robust presentation by adding redundant bits.
There are mainly two procedures for channel coding:

\begin{itemize}
	\item Forward error control: information bits are protected against errors by the transmitting of extra redundant bits, so that if errors occur during transmission the redundant bits can be used by the decoder to determine where the errors have occurred and how to correct them
	\item Automatic repeat request: In this method redundant bits are added to the transmitted information and are used by the receiver to detect errors. The receiver then signals a request for a repeat transmission
\end{itemize}

Some well known procedures are \emph{Hamming code, Convolutional encoding, Turbo code,...}
Again in order to succesfully apply these channel coding schemes, both the transmitter 
and the receiver must know which method is used.

Lastly, the channel encoded symbols $\ubar{Y}$ are modulated into a carrier wave 
to be transmitted over the \emph{channel}. Since the channel is always bandlimited to 
a certain range of frequency, the signal needs to be transmitted using a carrier that 
has that frequency. 
There are three basic modulation schemes:

\begin{itemize}
	\item Frequency modulation
	\item Amplitude modulation
	\item Phase modulation
\end{itemize}

A signal has three basic properties at every time $t$: frequency, amplitude and phase.
So, information can be modulated into a carrier wave by one, or more of this properties 
to represent the information to be transmitted. 
In this report, we will concentrate on an advanced method, called \emph{Quadrature amplitude modulation}, where both the amplitude and the phase of the carrier wave are changed to modulate information into the carrier wave.
This way, bits of information can be modulated into a two dimensional grid with one 
axis being the possible amplitude values and the other being possible phase values. 
As an example, defining 4 possible phase values and 4 possible amplitude values, the carrier wave can be changed to results in $4 \times 4$ different symbols that could be transmitted via this scheme. Thus an information stream of 4 bits can be modulated at once using 
this scheme.

There are also multiple modulation schemes each aiming at finding the most efficient and 
error resistant way to modulate information into a signal considering the probabilistic 
probabilities of the noise in channel.

\subsubsection{Channel}
 
The carrier wave $u(t)$ into which the information is modulated is now transmitted 
through the channel. It is important to note that, this part of the communication system
is the only part, where the outcome is not deterministic. 
By sending the carrier wave through air as it is done in terrestrial communication 
systems, other signals interfer with the carrier wave and might disrupt its inherent 
information. This is mostly always modelled with gaussian noise being added to the 
carrier wave $v(t) = u(t) + r(t)$ with $r(t)$ representing the noise \& interference in 
\ref{fig:BlockCommunicationSystem}.

Modelling the channel will later play an important part when applying \emph{deep learning}
methods to realize a communication system \ref{sec:applying_deep_learning_to_communication_systems}

\subsubsection{Receiver}

The receiver will then receive $v(t)$, inherently having a probabilistic nature due to the added gaussian noise.
The demodulator will then extract the information of the carrier wave $v(t)$ using 
the predefined values the information was modulated into (amplitude, phase). 
The demodulated stream of information $\ubar{Z}$ will most 
likely contain some errors due to the probable interference in the channel.
But having added redundant information using a channel encoding scheme, we can now use 
the scheme to correct possible errors, which include methods such as \emph{Minimum 
distance decoding, Syndrome decoding, Maximum Likelihood Decoding,...}.
The exact working goes too deep into coding theory which is not of big importance in 
this report, but can easily be read up on in common coding literature.
We now have, depending on the channel coding methods we chose, a good estimate of the 
original information $\ubar{\hat{X}}$
Finally, inverting the source encoding scheme, we can decode $\ubar{\hat{X}}$ to get 
$\hat{x}(t)$.

\subsection{Convential System Design}

As already stated in \ref{sec:motivation}, conventional communication systems use 
a chain of individual blocks each being individually chosen and optimized. 

The chosen method for the first block - source encoding - strongly depends on the proberties of the set of symbols the information to be sent is made up of. Thus, this method is often dynamically changed depending on what information is to be sent.
Channel encoding and modulation design is much more dependent on the given channel 
properties. Also, the trade off between error resistant and efficiency of the sent 
data structures in terms its average codeword length (number of bits), greatly influences
the methods to be used in channel encoding and modulation. 

In this report, we will use a conventional communication system employing binary
phase-shift keying modulation and a Hamming (7,4) code for comparison with systems 
applying deep learning methods.
A Hamming (7,4) code encodes four bits of data into seven bits for error resiliance. 
Doing so, the \emph{Hammming algorithm} can correct any single-bit error, or detect all single-bit and two-bit errors.
Phase-shift keying modulation is a digital modulation process modulating the phase of 
the carrier wave at precise time intervals.  

\section{Deep Learning Basics}%
\label{sec:deep_learning_basics}

This section aims at introducing the reader to the most important concepts of 
deep learning and how the can be applied to model communication systems.

\subsection{Deep Neural Network}%
\label{sub:deep_neural_network}

Deep neural networks are the essential building blocks in deep learning. 
They are very powerful mathematical models which aim to map some input $x$ to an 
output $f_{\theta}(x)$ by learning parameters $\theta$. 
In other words, they find parameters $\theta$ such that $f_{\theta}(x)$ is the best 
approximation of some function $f^¿$ \cite{Goodfellow-et-al-2016}. 
This function can describe many real-world tasks, e.g. classification of objects on a 
image, transcription of a spoken sentence or translation of a written sequence. 
This approximation power of deep neural networks accounts for their current success in a
variety of different applications.

Deep neural networks compose together many different functions (called neurons, nodes or units) in a directed acyclic graph manner. Output of each unit $y_j$ can be represented as a weighted sum of $N$ input units $x_i$ followed by a non-linear function $g$:
\[
	y_j =g(\sum_{i=1}^N x_i w_{j,i}+b_j),
\]
where $b_j$ is the bias term leading to the output node $y_j$ and $w_{j,i}$ is weight 
connecting the input node $x_i$ with the output node $y_j$.
Inside deep neural networks, information flows only in one direction, from the input x 
through hidden units to the output $f_{\theta}(x)$ without any loops. 
Hidden units in the network are organized in layers. Stacking more layers results in more abstract, high-level representations of the input data. 
The adjective ``deep'' thus symbolizes the ability of deep neural networks to 
``learn'' functions of increasing complexity by adding more layers in the network.

The so-called ``learning'' of deep neural networks $f_{\theta}$ refers to the optimizing of its parameters $\theta$ to learn a certain function. 
This is done, by defining a loss or cost function $L(y^{*},\hat{y} = f_{\theta}(x)) \in \mathbb{R}$.
In deep learning, trainings data is defined as a tuple $(x,y^{*})$ with the input data $x$
and its corresponding label $y^{*}$. The goal in the ``training'' procedure is 
then to find parameters $\theta$ that minimize the loss function $L$:
\[
	\text{argmin}_{\theta} L(y^{*},\hat{y}),
\]
for the training data by minimizing the differenc between $\hat{y}$ and $y^{*}$. 
Due to the high number of non-linear activation functions in a deep neural networks,
this optimization problem is almost always a non-convex problem, thus making it 
impossible to find an analytical solution for $\theta$.
Therefore, in training deep neural network algorithm like \emph{gradient descent}
are used trying to find a ``good'' local optimum \cite{Goodfellow-et-al-2016}.

Since encoding raw data to a more error resiliant and efficient form is essentially a complex
function $s: d \to d^*$, deep neural networks can be learned to approximate such 
a function $s$.

\subsection{Autoencoder}%
\label{sub:autoencoder}

An autoencoder is an artificial neural network that maps original data to reconstructed data obtained from 
a hidden representation. It contains a hidden layer $h$ (also denoted as coding or code 
layer) which describes an efficient coding of the input data $x$: $h = f(x)$. The function 
$h$ is called an encoder function and is trained to capture useful properties of the data.
The second part of an autoencoder is a decoder function $r = g(h)$ that aims to reconstruct
the original data. However, perfect reconstruction of the dataset is not the goal of the
autoencoder network. On the contrary, it is trained to be unable to just copy the input 
to output, but to extract useful information about data distribution instead \cite{Goodfellow-et-al-2016}.
Figure \ref{fig:autoencoder1} presents a compact view of an autoencoder.

\begin{figure}[htpb]
	\centering
	\includegraphics[width=0.2\linewidth]{images/autoencoder1.png}
	\caption{Structure of an autoencoder}
	\label{fig:autoencoder1}
\end{figure}

Autoencoders have been studied extensively in the literature in the recent years.
Several ways were proposed to obtain useful representations in a hidden layer $h$. 
One possible solution is to restrict the coding to have smaller dimension than the
original data. This type of autoencoder is called undercomplete. It learns a 
low-dimensional manifold that represents the principal sub-space of the training data. 

However, when the complexity of encoder and decoder functions is high, undercomplete 
autoencoders fail to learn salient features of the data and just perform useless identity function. To overcome this problem another family of models - regularized autoencoders - were developed. They provide the ability to choose the coding dimension and model flexibility with respect to complexity of the given data.
One of the most popular regularized autoencoders is the sparse autoencoder. It introduces a sparsity penalty $P(h)$ on the hidden layer $h$ that forces a model to learn unique 
statistical properties of the dataset in addition to an identity function. Training criterion of a sparse autoencoder will thus include both reconstruction error and sparsity
penalty:

\[
 L(x,g(f(x)))+P(h)	
\] 

Another way to force an autoencoder to learn useful features is to directly change the
reconstruction error. Denoising autoencoders minimize the objective function:

\[
	L(x, g(f(\widetilde{x}))
\]

where the input to the network $\widetilde{x}$ is a copy of the original input $x$ corrupted by noise. 
The goal of the autoencoder is therefore to recover the original input data from the corrupted signal. 
To achieve this goal denoising autoencoders implicitly learn underlying structure of the data distribution.

The relation to communication systems as they were defined in \ref{sub:structure_of_the_communication_system} is obvious: The blocks belonging to the transmitter are modeled by 
the encoder function $f: x \to h$ and the blocks belonging to the receiver are modeled 
by the decoder function $g: h \to r$. Since the output of the last encoded layer of the transmitter model $h$ presents the actual bit structure to be sent over the network, some kind of corruption or distortian has to be applied to the 
hidden layer $h$ modelling 
the probabilistic nature of the real channel \cite{synch1, DBLP:journals/corr/OSheaH17}.
Therefore, an autoencoder applied for the physical layer would have $f: x \to h$ as 
the encoding function of the transmitter network and $g: \widetilde{h} \to r$ with $\widetilde{h}$ being a corrupted/distorted version of $h$ as the decoding function of 
the receiver network. The transformation $c: h \to \widetilde{h}$ represents the 
channel network and imitiate the behavior of a ``real'' channel.
Figure \ref{fig:communicationSystemAsAutoencoder} presents an example of such a model.

\subsection{Generative Adversarial Networks}%
\label{sub:generative_adversarial_networks}

Generative adversarial networks belong to the class of generative models and are
based on differentiable generator networks \cite{Goodfellow-et-al-2016}.
Generative adversarial networks basically consists of two networks: 
\begin{itemize}
	\item The generator network: $g_{\theta^{(g)}}: z \to x_{f}, x_{f},z \in \mathbb{R}$
	\item The discriminator network: $d_{\theta^{(d)}}: x \to p, p \in \left[0,1\right], x \in \mathbb{R}$
\end{itemize}

The idea of generative adversarial networks is based on a game theoretic scenario in 
which the \emph{generator} $g_{\theta^{(g)}}$ network must compete against its adversary, the discriminator $d_{\theta^{(d)}}$ network.

The generator network takes random values $z$ as an input to produce ``fake data'' $x_{f}$.
The discriminator takes this ``fake data'' $x_f$ or real data $x_r$ as input $x$ and emits a 
probability value $p$ indicating whether the data is a real training example or not.

Having this setup, the goal of the generator is to produce data that will be 
evaluated as real data by the discriminator, while the goal of the discriminator 
is to distinguish the ``fake data'' produced from the generator from the real 
training examples. 

Learning in generative adversarial networks can best be described 
as a \emph{zero-sum game}, continious situations in which each networks pay-off (or loss
function) is exactly balanced by the pay-off of the utility of the other network.
Generative Adversarial Networks comprise multiple different approaches and can have 
multiple interpretations \cite{Goodfellow-et-al-2016}, but the ``standard version'' is as follows:
It consits of alternatively training $g_{\theta^{(g)}}(z) = x_f$ to be similar to $x_r$ and $d_{\theta^{(d)}}(x) = p$ to be as close to 1 for $x=x_r$ and close to 0 for $x=x_f$ leads to the 
generator to data that is the more and more similar to real training examples.
Putting in mathematically, we define a pay-off $v_{\theta^{(g)},\theta^{(d)}}$ the 
discriminator tries to maximize and the generator to minimize.
The form of the pay-off $v_{\theta^{(g)},\theta^{(d)}}$ again strongly depends on the 
model, but is often of the form:
\[
	v_{\theta^{(g)},\theta^{(d)}} = \mathbb{E}_{x_r}\log (d_{\theta^{(d)}}(x_r)) + \mathbb{E}_{x_f} \log (1 - d_{\theta^{(d)}}(x_f))
\]
which can also be written as:
\[
	v_{\theta^{(g)},\theta^{(d)}} = \mathbb{E}_{x_r} \log (d_{\theta^{(d)}}(x_r)) + \mathbb{E}_{z} \log (1 - d_{\theta^{(d)}}(g_{\theta^{(g)}}(z)))
\]

Since our final goal is to have a generator function that is able to produce 
data being indistinguishable from real training data, we define $\theta^{(g^{*})}$:
\[
	\theta^{(g^{*})} = \text{argmax}_{\theta^{(g)}} max_{\theta^{(d)}} v_{\theta^{(g)},\theta^{(d)}}
\]

At convergence, the discriminator has to output $p=\frac{1}{2}, \forall x \in \{x_r,x_f\}$.

Unfortunately, training general adversial networks can be quite difficult. 
As stated in \cite{Goodfellow-et-al-2016}, convergence in a \emph{zero-sum game} (or minmax game) is not a local optimum in $v_{\theta^{(g)},\theta^{(d)}}$, but ``that are simultaneously minimafor both netwokrs' costs''.
These points are saddle points in $v_{\theta^{(g)},\theta^{(d)}}$ being 
local minima with respect to the first network parameters and maxima with respect to 
the second network's parameters. 
Multiple approaches have been realized trying to overcome this problem, that 
can be read up on \cite{Goodfellow-et-al-2016}

Generative adversarial networks have very succesfully applied to generate images 
that look indistinguishable to real images to the human eye in some case 
\cite{7550880}. 
As always in machine learning methods, the more and better data we have for 
training, the better are the results generated by generative adversarial networks.

In section \ref{}, we will explain in detail how generative adversarial networks can 
be applied in autoencoders for communication systems to imitiate a ``real'' channel 
response.

\section{Applying deep learning to communication systems}%
\label{sec:applying_deep_learning_to_communication_systems}

The section is the core of the report. Using the models and methods,
we defined in the earlier sections, a communication systems consisting 
only of artificial neural networks will be described here in detail. 

To begin with, the structure and methodology of these communication systems will be presented
in detail \ref{sec:artificial_neural_network_supported_end_to_end_systems}.

There are two general problems when modelling end-to-end communication systems
with artificial neural networks.
\begin{enumerate}
	\item The channel response needs to be included in the model and thus 
	      modelled by a function trying to imititate the real world behaviour 
	      of a channel response
	\item 
\end{enumerate}

Conventional communication systems optimize each block of the system \ref{fig:BlockCommunicationSystem} independently. Therefore it does not need to model the channel 
response: Transmitter and Receiver are optimized independently. In artificial neural 
network supported end-to-end approaches, the system as a whole is optimized, meaning 
that in the receiver parameters of the receiver affects the paramaters of 
the transmitter. Thus, when applying gradient descent \ref{sec:deep_learning_basics} 
the gradient needs to be able to ``flow'' from the output of the end of the network 
to the beginning of the network making it absolutely necessary to 
model the connection between transmitter and receiver - the channel - by another 
layer of the network (see \ref{fig:communicationSystemAsAutoencoder}).
Section \ref{ssub:generative_adversarial_networks_based_communicaton_system} tries 
to overcome this problem by using \emph{generative adversarial networks}.

\subsection{Artificial neural network supported end-to-end communication systems}%
\label{sub:artificial_neural_network_supported_end_to_end_systems}

\begin{figure}[htpb]
	\centering
	\includegraphics[width=1\linewidth]{images/communicationSystemAsAutoencoder.png}
	\caption{End-to-end communication system as an autoencoder}
	\label{fig:communicationSystemAsAutoencoder}
\end{figure}

Describe here two approaches: the base approach where the channel 
is just modeled by a gaussian noise layer \cite{2018ISTSP..12..132D} and \cite{DBLP:journals/corr/OSheaH17}
and the more sophisticated approach using Generative Adversarial Networks \cite{2018arXiv180506350O} and \cite{2018arXiv180303145O}.
Describe architectures and training in detail.

\subsubsection{Generative Adversarial Networks to model the channel}%
\label{ssub:generative_adversarial_networks_based_communicaton_system}

\subsection{Synchronization using artificial neural networks}%
\label{sub:synchronization_problem_when_using_artificial_neural_networks}

\cite{synch1}
\cite{synchAttention}

\subsection{Comparison deep learing methods and conventional methods}

Compare current ``state-of-the-art'' to system using deep learning.

\subsubsection{Performance measurement}

Define and introduce Signal-to-noise ration

\subsubsection{Results using deep learning methods}

State attained results. 

\section{Conclusion} % (fold)
\label{sec:conclusion}

In which cases could deep learning be applicable for communication systems
Conclude by trying to answer the question, whether it is useful 
to apply deep learning methods for communication systems (or under which 
circumstances it might be useful)

% section conclusion (end)
\newpage

\addcontentsline{toc}{section}{References}
\bibliographystyle{plain}
\bibliography{vonPlaten_report}

\end{document}
