\documentclass[twoside,11pt,a4paper,english]{article}

% packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx,curves,float,rotating}

\usepackage{amsmath, amssymb, latexsym}  % math stuff
\usepackage{amsopn}                             % um mathe operatoren zu deklarieren
\usepackage[english]{babel}                     % otherwise use british or american
\usepackage{theorem}                            % instead of \usepackage{amsthm}
\usepackage{dcolumn}
\usepackage{hyperref}
\usepackage[]{algorithm2e}
\usepackage{float}


% @ environment %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{xspace}                             % context sensitive space after macros
\makeatletter 
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{{e.g}\onedot} \def\Eg{{E.g}\onedot}
\def\ie{{i.e}\onedot} \def\Ie{{I.e}\onedot}
\def\cf{{c.f}\onedot} \def\Cf{{C.f}\onedot}
\def\etc{{etc}\onedot} \def\vs{{vs}\onedot} 
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{{et al}\onedot}
\def\zB{z.B\onedot} \def\ZB{Z.B\onedot}
\def\dh{d.h\onedot} \def\Dh{D.h\onedot}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%	Macros fuer neue Umgebungen
  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand*{\Frac}[2]{\frac{\displaystyle #1}{\displaystyle #2}}
\newlength{\textwd}
\newlength{\oddsidemargintmp}
\newlength{\evensidemargintmp}
\newcommand*{\hspaceof}[2]{\settowidth{\textwd}{#1}\mbox{\hspace{#2\textwd}}}
\newlength{\textht}
\newcommand*{\vspaceof}[3]{\settoheight{\textht}{#1}\mbox{\raisebox{#2\textht}{#3}}}
\newcommand*{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcommand{\reals}{\mathbb{R}}

\newenvironment{deflist}[1][\quad]%
{  \begin{list}{}{%
      \renewcommand{\makelabel}[1]{\textbf{##1}\hfil}%
      \settowidth{\labelwidth}{\textbf{#1}}%
      \setlength{\leftmargin}{\labelwidth}
      \addtolength{\leftmargin}{\labelsep}}}
{  \end{list}}


\newenvironment{Quote}% Definition of Quote
{  \begin{list}{}{%
      \setlength{\rightmargin}{0pt}}
      \item[]\ignorespaces}
{\unskip\end{list}}


\theoremstyle{break}
\theorembodyfont{\itshape}	
\theoremheaderfont{\scshape}

\newtheorem{Cor}{Corollary}
\newtheorem{Def}{Definition}
%\newtheorem{Def}[Cor]{Definition}



\newcolumntype{.}{D{.}{.}{-1}}


\pagestyle{headings}
\textwidth 15cm
\textheight 23cm
\oddsidemargin 1cm
\evensidemargin 0cm
%\parindent 0mm



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%       Jetzt geht's los
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%               Title
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{empty}

\begin{center}

    Rheinisch-Westf\"alische Technische Hochschule Aachen \\
    Lehrstuhl f\"ur Informatik 6 \\
    Prof. Dr.-Ing. Hermann Ney\\[6ex]
    Seminar Titel im SS 2018\\[12ex]                          % auch Seminar Titel und Datum ändern!!!
   
    \LARGE
    \textbf{Deep Clustering for ANN Supported Source Separation } \\[6ex]
    \textit{Patrick von Platen} \\[6ex]
    \Large
    Matrikelnummer 331 430 \\[6ex]
    Datum des Vortrages

    \vfill
    \Large Betreuer: Tobias Menne 
	    
\end{center}

\newpage
\ 
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%               Inhaltsverzeichnis / Tabellenverzeichnis / Abbildungsverz.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{headings}
\tableofcontents
\listoftables
\listoffigures
\newpage
\pagestyle{empty}
\ 
\newpage
\pagestyle{headings}

\section{Introduction} % (fold)
\label{sec:introduction}

When perceiving sound created by multiple acoustic sources, the human auditory system
is extremely good at focussing on parts of the sound coming from only one acoustic
source.

Imagine yourself at a cocktail party. The sound that you perceive is made up of all kinds of acoustic sources: The voice of the person you are listening to, the sound of music,
the background chatter of other people talking, the sound of clinking glasses, etc. Still you can clearly understand the speaker in your group, even though its acoustic signal overlaps at every moment with the acoustic signal of other people speaking and background noise. 

The difficulty of understanding speech in a multiple speaker environment is often called "The cocktail party problem" \cite{CocktailPartyProblem:2000}, first mentioned by Colin Cherry in 1953. 
It was also Colin Cherry, who first brought up the idea of automatic speech separation in his famous paper on ``experiments of the recognition of speech'' \cite{Cherry:1953}.
The first method to yield some promising results was introduced by A.S. Bregman in 1990 using \emph{Auditory scene analysis} \cite{bregman1994auditory}.
Since then, multiple methods have been explored. One of them is \emph{Spectral Clustering}
, a method based on partitioning data points into clusters according to eigenstructure
of a similarity matrix. 

With the receent rise of deep learning in a variety of applications, the first method
to be applied to source separation was presented by Chao Weng and co. in 2015 \cite{SpeechSepDeepLearning:2015}. 
Quickly, different deep learning methods based on a technique called \emph{Deep Clustering} emerged, forming the state-of-art in the source separation problem. 
This report, will focus on the deep clustering method as it was introduced by John R. Hershey and co. in 2015 \cite{BasicDeepClustering:2016} and will study \emph{TasNet}, a deep clustering system performing audio separation in the time domain yielding state-of-the art results \cite{TasNet}.

The applications of automated source separation are wide-ranging. To name a few: 
\begin{itemize}
	\item \textit{Automatic meeting transcription}: In business meetings, multiple speakers are alteranating in a short time or even speaking at the same time. Making it possible to automatically separate speakers from each other and transcribe would be of great use for companies. 
	\item \textit{Virtual assistant}: Virtual assistants, like Alexa, Siri and Google Home are playing a critical part in smart home systems. Filtering out noise and separating the owner's voice from other voices are challenges that can be taken on by automated source separation techniques. 
	\item \textit{Automatic subtitling of music/video}: According to the World Health Organization (WHO) over 5\% of the world population suffers from disabling hearing loss \cite{HearingLoss}. Subtitling of music and video content, which are often made up of sound coming from multiple acoustic sources, is therefore essential for them.	
\end{itemize}

To begin with, the source separation problem is mathematically defined and basic mathematical operations and basic deep learning methods used in deep clustering are explained \ref{sec:basics}.
In the following conventional methods and first attempts of using deep learning techniques for automated source separation are presented \ref{sec:conventional_methods}.
		The essence of this paper being deep clustering and \emph{TasNet} will be presented in-detail in section \ref{sec:deep_clustering_methods}. 
Finally, a conclusion is drawn \ref{sec:conclusion}.
% section introduction (end)

\section{Basics} % (fold)
\label{sec:basics}

This section firstly defines the source separation problem and then gives an 
overview of the basic operations and methods used in systems.

\subsection{Definition source separation problem} % (fold)
\label{sec:source_separation_problem}

A generel definition of the source separation problem was given by Jean-Francois Cardoso, 
defininig the problem as ``recovering unobserved signals or sources from several
observed mixtures'' \cite{Cardoso03blindsignal}. He also added two important restrictions that we will apply to our definition as well: 
\begin{enumerate}
 \item the source signals are not observed
 \item no information is available about the mixture
\end{enumerate}

The condition of not knowing how many sources the mixture is composed of, became known as \emph{blind} source separation \cite{Cardoso03blindsignal}. 
In this paper, we add another restriction: The signal is observed over a single channel (single microphone).
To sum it up, the \emph{source separation problem} is defined in this paper as ``recovering the original signals of an unknown amount of sources from a single-channel mixture of the source signals''.

Putting the above definiton in a mathematical form:
\begin{itemize}
	\item the unknown number of acoustic sources: $N$.
	\item the original sound of the i-th source at time t: $x_i(t)$.
	\item the single received sound signal at time t: $X(t) = \sum_{i=1}^{N} x_i(t)$.
\end{itemize}

The goal of source separation is to recover $x_1(t),...,x_N(t)$ from $X(t)$.
An algorithm performing this task is presented below.

\begin{algorithm}[ht]
	\RestyleAlgo{boxruled}
	\LinesNumbered
	\textbf{Input: }$X(t)$ \\
	\textbf{Output: }$(N, \{x_1(t), ..., x_N(t)\})$
\end{algorithm}

% section source_separation_problem (end)


\subsection{Mathematical operations} % (fold)
\label{sub:mathematical_oprations}

This subsection aims to shine light on essentiel 
mathematical operations used in the systems that will 
be explained in sections \ref{sec:conventional_methods}, \ref{sec:deep_clustering_methods}. 

\subsubsection{Short term fourier transform} % (fold)
\label{ssub:short_term_fourier_transform}

In order to understand the short term fourier transform, one firstly has to get an understanding of the fourier transform itself. Having a continous signal $s(t)$, the fourier transform is defined as

\[
	S(f) = \int_{\mathbb{R}} s(t) e^{-2j{\pi}ft} dt
\]

The fourier transform $S(f)$ gives the magnitude of every frequency in the signal. A signal can be decomposed as the sum of signals consisting only of one frequency (so called sinusoids).
 Thus, one can say that the higher the magnitude of a frequency of the fourier transform, the higher is the percentage of the signal being composed of the sinusoid of
 this frequency. An important aspect to notice is that the fourier transform
 integrates over the whole time domain of the signal in order to get the exact magnitude
 of every freuency present in the signal. Thus integrating only over a part of the time domain leads
 to approximated values of the frequencies magnitudes. On the other hand, a shorter time
 intervall to integrate over leads to a better time localization of the frequency. This famous
 trade-off is well-known as the uncertainty principle in signal analysis \cite{DBLP:journals/corr/Nam13}.

The short term fourier transform divides the signal into local sections using a window function $w(\tau) = 0 : \tau \not \in \left[-a, a \right]$ over which the fourier transform is applied.  
\[
	S(f,t) = \int_{\mathbb{R}} s(\tau)w(\tau-t) e^{-2j{\pi}f\tau} d\tau 
\]

Appling the short term fourier transform converts a signal $X(t)$ to multiple $(F,T)$ binsthat can be plotted in a \emph{spectrogram}. It therefore is used to describe the change in frequency over time. Using an appropiate window funciton leads to a good trade off between time localization and frequency approximation.

% subsubsection short_term_fourier_transform (end)

\subsubsection{Embedding space} % (fold)
\label{ssub:embedding_space}

An embedding is a structure-preserving, injective mapping $f: X \to Y$. Therefore, every
object of domain $X$ is mapped to a distinct object in domain $Y$ . In this report, we are
interested in embeddings that map to normed spaces meaning that it is possible to measure
the similarity of two objects $x_1$ and $x_2 \in X$ by measuring its distance $d(y_1; y_2) \in \mathbb{R}: y_1, y_2 \in Y$.
It is essentially used to convert data to a feature representation where certain properties
can be preserved by distance, so that the data is easily comparable by computers.

One famous example are word embedding models, such as \emph{word2vec} that maps words
to a vector representation. Assuming that words have multiple degrees of similarity, word
embeddings are used to easily calculate the similarity of words on multiple axis \cite{DBLP:journals/corr/abs-1301-3781}.
Later, we will see that the function mapping objects from one domain to an embedding
space can be trained on artificial neural networks and are very powerful tools for the source separation problem \ref{sub:frequency_domain_audio_separation}.

\subsection{Deep neural network basics} % (fold)
\label{sub:deep_neural_network_basics}

This subsection aims to clarify basic deep learning methods and architectures used in the sections \ref{sec:conventional_methods}, \ref{sec:deep_clustering_methods}.

\subsubsection{Recurrent neural network} % (fold)
\label{ssub:recurrent_neural_network}

Recurrent neural networks are neural networks that are ``specialized in processing a sequence
of values $x_1,...,x_t$'' \cite{Goodfellow-et-al-2016}. For that reason, they are very well applicable when dealing
with acoustic signals having high autocorrelation (high correlation between $s(t)$ and
$s(t - k, s(t + k)$  for $k > 0$).
Recurrent neural networks are extremely effective when dealing with sequential data.
First, they process data of different time instances in one computation
step, thus taking full advantage of autocorrelation. Second, they can handle data
sequences of variable length in contrast to feed forward neural networks where the input length is determined by the size of the input layer. 
This is achieved by loops between
recurrent units inside the neural network \cite{Goodfellow-et-al-2016}.
Unfolding a recurrent unit $z$ of a recurrent neural network at different time instances
leads to so-called hidden copies of the network being connected to each other: $z^0, z^1, ..., z^{t-1}, z^{t}$
As a result, such a recurrent unit $z^t$ at time t takes both the output of the node leading
to it ($x^t$) and the output of the same recurrent unit of one time instance earlier $(z^{t-1})$ as input resulting in:

\[
	z^t =f(V x^t + Wz^{t-1} + b)
\]
with $f()$ being the activation function, V the weights between recurrent unit z and input
x, W the weights between two hidden recurrent units $z^{t-1}$ and $z^t$ and b the constant bias term.
This basic setup is known to have the vanishing/exploding gradient problem due to an
exponential decrease/increase in value of long-term components ($\frac{\partial z^t}{\partial z^{t-k}} \to \text{ \textit{const.}} \times W^{k} \to 0 \text{ or } \infty$
)\cite{Pascanu:2013:DTR:3042817.3043083}. To overcome this problem, Hochreiter and co. introduced a ``better'' recurrent unit,
called long shot term memory.

% subsubsection recurrent_neural_network (end)

\subsubsection{Long short term memory} % (fold)
\label{ssub:long_short_term_memory}

To overcome the exploding/vanishing gradients problem, the long short term memory unit
introduces an internal state which can be seen as the unit's ``memory'' \cite{Hochreiter:1997:LSM:1246443.1246450}. At every time
instance t, the unit updates its memory $c^t$ using the memory $c^{t-¿}$ and the unit's output
$h^{t-1}$, as well as the new input$x^t$. $c^t$ in combination with $h^{t-1}$ and $x^t$ is then used to
produce the new outputht. The structure of the long short term memory unit is shown in
Figure \ref{fig:lstm}. It is important to notice that the long short term memory uses $x^t$ and $h^{t-1}$ to
compute four different gates: f,i,g,o which are used to control the 
flow of information in
the unit. Also, we can see that when doing backpropagation, the gradient easily ``flows'' to
earlier hidden units without the weight matrix being applied to it (shown by the red arrow) in \ref{fig:lstm}.
This ``highway'' for the gradient strongly reduces the vanishing/exploding gradient
effect and allows the unit to take into account long term dependencies of the input data.

\begin{figure}[h!]
  \includegraphics[scale=0.4]{images/lstm_structure.png}
  \centering
  \caption{Long short term memory}
  \label{fig:lstm}
\end{figure}

To furter improve this ability, bi-directional long short term memory units were proposed
to exploit correlations between data in both time directions and are used in many
state-of-the-art systems nowadays. Going more into detail is out of scope for this report,
but more in-detail informationcan be found in \cite{Hochreiter:1997:LSM:1246443.1246450}.

% subsubsection long_short_term_memory (end)

\subsubsection{Autoencoder}

An autoencoder is a neural network that is ``trained to attempt to copy its input to its
output'' \cite{}. It always consists of two parts, the encoder and the decoder. In its simplest
form, the encoder maps the input x to one hidden layer $f(x) = z$ and the decoder tries to
reconstruct the original data $g(z) = x'$ from this layer.
Even though autoencoders are trained with loss functions trying to minimize the difference
between $x$ and $x'$, perfect recontruction of the data input is not the goal. Instead,
autoencoder should be trained in a way preventing them to just copy the input to the
output, but to extract useful information about data distribution instead \cite{Goodfellow-et-al-2016}. Thereby, the
hidden layer should represent the most useful information. Two approaches are considered
in this report.
First, a undercomplete autoencoder is defined by the hidden layer having much lower
dimension than the data, thus forcing the network to ``store all data in a compressed form''.
A backdraw of this approach is that the hidden layer might be two small to learn enough
useful information. Second, a sparse autoencoder forces the hidden layer to be sparse, by
introducing a sparsity penalty on the hidden layer $P(h)$, which will then be added to the
loss function $\mathbb{L}(x,x')+P(h)$. Especially in the field of machine translation, autoencoders
in combination with recurrent neural networks produce state-of-the-art results \cite{DBLP:journals/corr/abs-1801-05119}. 
For more detailed information, please refer to \cite{Goodfellow-et-al-2016}.

% subsection deep_neural_network_basics (end)
% section basics (end)

\section{Conventional methods} % (fold)
\label{sec:conventional_methods}

The first theoretical framework for automated source separation was introduced by A.S. Bregman in 1990 \cite{bregman1994auditory}. Systems using this approach are called \emph{computational auditory scene analysis} and produced first results, that are 
acceptable \cite{4429320}. Section \ref{sub:computational_auditory_scene_analysis} will 
handle cover the system. 

\emph{Spectral Clustering}, a method well-know for data clustering, was then applied 
to the source separation problem and showed good results
(see \ref{sub:spectral_clustering}). 
It was the first method used for source separation to train some of its parameters on actual data \cite{Bach:2006}. 

\subsection{Computational auditory scene analysis}%
\label{sub:computational_auditory_scene_analysis}

Computational auditory scene analysis for source separation is an automation of the auditory scene analysis 
which is inspired by the principles of processing sound in human auditory system \cite{4429320}. 
In computational auditory scene anlysis the algorithm consists of two parts: \emph{segmentation} and \emph{grouping}. A more ex
A detailed description of the algorithm can be seen in the following and it 
based on the paper of \emph{Wang} and \emph{Brown} \cite{4429320}.

\vspace{5mm}

\begin{algorithm}[H]
	\RestyleAlgo{boxruled}
	\LinesNumbered
	\textbf{Input: }(N, $X(t)$) \\
	\textbf{Algorithm: }
	\begin{enumerate}
		\item Segmentation:
			\begin{itemize}
				\item X(t) is transformed into time-frequency space 
				      using short term fourier transform
				\item Segmenation rules are used to define different
				      time-frequency (T,F) regions.
			\end{itemize}
	    	\item Grouping:
			\begin{itemize}
				\item $(F,T)$ bins are combined to streams corresponding to sound sources
				\item Auditory masks are created using different streams
			\end{itemize}
	\end{enumerate}
	\textbf{Output: }$(\{x_1(t), ..., x_N(t)\})$
\end{algorithm}

\vspace{5mm}


The segmentation rules used in the segmentation part are hand-crafted similarity 
features upon which the different (T,F) regions are created. 
For every speaker, an audiotry masks is created as a last step of the grouping part.
These auditory masks are boolean matrices that 
can be applied to the spectogram to retrieve the part of the spectogram 
corresponding to a speaker.
Important to notice is that the amount of different speakers N has to be an input 
of the algorithm and cannot be computed by the system itself. 
Since computational auditory scene analysis is a rule based system it is very 
difficult to generelize to broader conditions and needs very precise tuning in 
order to work.

Computational auditor scene analysis was the first version to produce 
some meaningful results, but is not powerful enough for the general source seperation
problem as we defined it in section \ref{sec:source_separation_problem}.

\subsection{Spectral Clustering} % (fold)
\label{sub:spectral_clustering}

\emph{Spectral Clustering} is a well-known method in multivariante statistic for the clustering of data. It makes use of the eigenvalues (also called spectrum, thus the name 
\emph{Spectral clustering}) of a similarity matrix $W$ of the data to reduce the dimensionality of the data.
Since spectral clustering will be very important in latter chapters it will be described here in detail. 
The similarity matrix $W$ can be defined in multiple ways depending on the task, but it should have the following properties.

\begin{enumerate}
	\item W should be a hermitian matrix: $W^T = W$.
	\item $(W)_{i,j} \ge 0, \forall i,j$
	\item W should be non-negative definite.
\end{enumerate}

Considering a dataset of P entries $D \in \mathbb{R}^P$, describes the similarity between
each of these datapoints, thus $W \in \mathbb{R}^{P \times P}$. Since $W$ can be considered as a distance measure, whereas $W(d_1,d_2) = W(d_2,d_1$ and with only positive distances, the first two of the above defined properties seem reasonable.
The third property ensures that all eigenvalues of $W$ are $ \ge 0$ being a characteristic
of non-negative definite matrices.

Having defined a similarity matrix, an example of an algorithm for spectral clustering used in \cite{Bach:2006} is presented in figure \ref{fig:spectral_clustering_algo}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{images/spectral_clustering_algo.png}
	\caption{Spectral clustering algorithm}
	\label{fig:spectral_clustering_algo}
\end{figure}

First, all eigenvalues and their corresponding eigenvectors of the similarity matrix are calculated. 
The eigenvectors $v_i \in \mathbb{R}^P$ can be sorted according according to their eigenvalues
in descending order, such that $v_0, v_1, ..., v_r,...,v_p$ with $\lambda(v_0) = max_{i \in \{0,...,P\}} \lambda_i $.
Now we can choose the first R eigenvectors to have R different clusters.  
Our set of eigenvectors is defined as $V = (v_0, v_1, ..., v_r) \in \mathbb{R}^{P \times R}$. We can now define a set $U = (u_0, u_1, ..., u_p) = V^T \in \in \mathbb{R}^{R \times P}$, whereas $u_i$ corresponds to exactly one data point and is the vector containing the i-th elements of all eigenvectors $v_1,...,v_r$.
As a last step, we apply a weighted version of the \emph{k-means algorithm} \cite{Steinhaus56} by first randomely selecting R vectors $m_1,...,m_r$ as mean vectors and then successively assigning the vectors $u_1,...,u_p$ and updating $m_1,...,m_r$.
After conversion, all vectors assigned to $m_j$ present the j-th cluster.

Bach and Jordan applied spectral clustering for source separation of two-speaker mixtures.
The method described in the following is based on their paper ``Learning spectral clustering, with application to speech separation'' \cite{Bach:2006}.

First, the definition of a similarity matrix $W$ corresponding to the mixture signal $X(t)$ will be derived.
As a first step the mixture signal can be convert to $(F,T)$ bins using the short term 
fourier transform as explained in \ref{ssub:short_term_fourier_transform}.
The $(F,T)$ are then used to create multiple features, which are based on: \emph{Energy,
continuity, common fate cues, pitch estimation, timbre,...}. In this way, we create multiple features $s_i(f,t)$ for every data point $(f,t)$ whereas the calculation of $s_i(f,t)$ can make use of neighoring data points. 

The features are essentially embeddings making the data points more comparable, so that we can define k similarity matrix corresponding to k features $W_{k}(i,j) = f_k(s_i, s_j)$ choosing appropriate functions $f_k$. 
The final similarity matrix consists of a weighed combination of each similarity matrix $W = W_1^{\alpha_1} \odot W_2^{\alpha_2} \odot ... \odot W_k^{\alpha}$ with $\odot$ being the hadamard product.
The similarity matrix can then be inserted into our previously explained spectral 
clustering algorithm \ref{fig:spectral_clustering_algo} and the two clusters can be defined resulting in a neat spectogram for every speaker. The conversion of the data points to 
a combined similarity matrix works like front-end processor. 

The only thing left to explain is how to define the weigth vector $\alpha \in \reals^K$.
Having N labeled data sets with the $(F,T)$ bins correspondng to one of two speakers 
so that every data set has a reference partition $E_n'$, a partition $E_n(\alpha) = \text{K-Means}(W_n(\alpha))$ can be derived. 
The \emph{k-means} algorithm is applied to the similarity matrix depending on $\alpha$ and the previously defined features as well as the \emph{k-means} algorithm.
Finally, a cost function $\mathbb{L} = \sum_i^N H(E_n',E_n(\alpha))$ is used to find the $\alpha$ minimizing the $\mathbb{L}$. 

Even though spectral clustering can produce very good results on two-speaker separation \cite{Bach:2006} it has its limitations. 
Already for seconds of speech sampled at 5.5 kHz leads to 22000 data points and thus, a similarity matrix having $22000 \times 22000$ entries not even applying the short term 
fourier transform. It is obvious that eigenvalue and eigenvector calculations become very
expensive. 
A remedy is applying low rank matrix approximations such as the Nyström approximation to perform eigenvalue and 
eigenvector on a matrix that is reduced from $N \times N$ to $N \times D$. 
This allows for significant performance improvement and is especially useful on sparse matrices \cite{Bach:2006}.

Additionally, the algorithms relies on ``handcrafted features'' and uses only shallow learning for the similarity matrices by essentially applying linear regression to optimize 
the scaling vector $\alpha$. 
More speakers means more overlapping speakers per $(F,T)$ bin so that features taking into
account multiple data points for calculation contain multiple speakers making it 
pointless to cluster them. A speaker segmentation would be needed before-hand to 
create useful features, which would miss the point of creating features to perform 
the segmentation. Deep clustering as explained in \ref{sub:frequency_domain_audio_separation} tries to solve this problem.

% subsection spectral_clustering (end)

% section conventional_methods (end)

\section{Deep clustering methods} % (fold)
\label{sec:deep_clustering_methods}

In this section, two state-of-the-art approaches using artificial neural networks for the source separation problem will be discussed. 
The first approach, called ``Deep clustering'' \ref{sec:frequency_domain_audio_separation} is very similar to \emph{spectral clustering} as explained in the section before and works in the frequency domain of the signal. 
the second approach, called ``TasNet'' works in the time domain and will be discussed in detail in 
\ref{sub:time_domain_audio_separation}.

When applying artificial neural networks to the source separation problem there are two common problems that 
occur. 

\begin{itemize}
	\item The permutation problem 
	\item The dimension output problem 
\end{itemize}

To best understand the permutation problem of artificial neural networks, let us go through an example.
An artificial network for two-speaker source separation that takes n $(F,T)$ bins corresponding to the mixed signal $X(t)$ as an input in the first layer would need an output layer of size $2 \times n$ with the first half 
corresponding to the first speaker and the second half to the second speaker. 
Having three two-speaker datasets, for example (A,B), (A,C) and (B,C) to train such a network the following problem 
occurs. First the network will learn to activate the $(F,T)$ corresponding to speaker A on the left half of 
the output layer and the output for speaker B on the right half. Second, the network learns to assign speaker A again to the left half and speaker C to the right half. 
Third, when training on the dataset of speaker B and C, the network will try to assign the right half to both speaker B and C \cite{SingleChannelSourceSeparation}. 
These constant ``mis-learnings'' will it most probably impossible for the learning process to converge.

The dimension output problem refers to the static input and output dimension of normal feed forward neural networks.
Having defined a output dimension corresponding to two speakers for example the network will learn to separate 
only two-speaker signal mixtures making it impossible to separate signal mixtures containing more than two 
speakers. 

The explanation of the two methods will be structured by answering the following questions.

\begin{enumerate}
	\item What is the idea of the method?
	\item How is the system trained? 
	\item How is the trained system applied to new data?
	\item How does the method solve the two problems described above?
\end{enumerate}

\subsection{Deep clustering method - frequency domain audio separation based methods} % (fold)
\label{sub:frequency_domain_audio_separation}

Method that assigns contrastive embedding vectors to each time-frequency region.
Present in detail as described in \cite{BasicDeepClustering:2016}.
Can be used for unknown arbitrary amount of sources. 

\subsubsection{Training} % (fold)

Talk about what to pay attention to when training.

\subsubsection{Results} 

talk about what ever

\subsection{TasNet - time domain audio separation based methods} % (fold)
\label{sub:time_domain_audio_separation}

Introduce TasNet which does not create masks for each source, but instead use encoder-decoder 
framework to model directly the signal in the time-domain. Use \cite{TasNet}.

\subsubsection{Training} % (fold)

Talk about what to pay attention to when training.

\subsubsection{Results}

Talk about results of TasNet

% subsection time_domain_audio_separation (end)
% section deep_neural_network_methods (end)

\section{Conclusion} % (fold)
\label{sec:conclusion}

Write conclusion using results.

\newpage

\addcontentsline{toc}{section}{References}
\bibliographystyle{plain}
\bibliography{vonPlaten_report}

\end{document}
